from fastapi import APIRouter, Query, HTTPException, Request, Response
from app.services import pinecone_client, embeddings, llm
from pydantic import BaseModel
from typing import Optional, Dict, List
import re
import os
from openai import OpenAI
from dotenv import load_dotenv
import json
from datetime import datetime
import uuid
import random

load_dotenv()

# In-memory conversation history storage (for demo purposes) need to use other db for context storage 
conversation_history: Dict[str, List[Dict]] = {}

router = APIRouter()

class ChatRequest(BaseModel):
    query: str
    session_id: Optional[str] = None

# get_topics_response() function has been removed as per your request.
    
def get_no_knowledge_response(language: str, user_query: str) -> str:
    """Dynamically generates a no-answer response with related topic suggestions using the LLM."""
    try:
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        
        
        prompt = f"""The user asked a question in {language} about: "{user_query}". I couldn't find a direct answer in my knowledge base. Generate 3-4 related topics from the Bhagavad Gita that I can offer as suggestions. The topics should be general and inviting. Respond in the same language.
        
        Example in English:
        Based on your question, I can tell you about:
        - The nature of the soul (atman)
        - The different paths of Yoga (Karma Yoga, Bhakti Yoga)
        - Krishna's role as a divine guide
        
        Generate a similar response for the user's query in {language}.
        """
        
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "user", "content": prompt}
            ],
            max_tokens=150,
            temperature=0.7 # Higher temperature for more creative suggestions
        )
        
        # Combine a random apology with the LLM-generated suggestions
        apology_responses = {
            'english': [
                "Sorry, I couldn't find verses for that. Maybe try asking in another way?",
                "Hmm, I don't see a reference for that. Want me to share something related instead?",
                "Apologies üôè, I don't have that exact answer. But I can guide you with related wisdom.",
                "Sorry, I couldn't find that specific information. However, I can help you with these related subjects:",
                "I apologize, but I couldn't find relevant information about this specific topic. However, I can help you with these related subjects:",
                "Sorry, I don't have that exact answer in my knowledge base. But I can tell you about these related topics:",
                "Hmm, I couldn't find that specific reference. Want to explore these related concepts instead?",
                "Sorry, I couldn't find the information you're looking for. But I can help you with these related subjects:"
            ],
            'telugu': [
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞à ‡∞®‡∞ø‡∞∞‡±ç‡∞¶‡∞ø‡∞∑‡±ç‡∞ü ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞®‡∞æ‡∞ï‡±Å ‡∞¶‡±ä‡∞∞‡∞ï‡∞≤‡±á‡∞¶‡±Å. ‡∞Ö‡∞Ø‡∞ø‡∞§‡±á, ‡∞à ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞µ‡∞ø‡∞∑‡∞Ø‡∞æ‡∞≤‡∞§‡±ã ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞∏‡∞π‡∞æ‡∞Ø‡∞™‡∞°‡∞ó‡∞≤‡∞®‡±Å:",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞µ‡±Ü‡∞§‡±Å‡∞ï‡±Å‡∞§‡±Å‡∞®‡±ç‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞®‡∞æ‡∞ï‡±Å ‡∞¶‡±ä‡∞∞‡∞ï‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞®‡∞ø‡∞∞‡±ç‡∞¶‡∞ø‡∞∑‡±ç‡∞ü ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞à ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞Ö‡∞Ç‡∞∂‡∞æ‡∞≤ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞ó‡∞≤‡∞®‡±Å:",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞¶‡±ä‡∞∞‡∞ï‡∞≤‡±á‡∞¶‡±Å. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞à ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞µ‡∞ø‡∞∑‡∞Ø‡∞æ‡∞≤‡∞§‡±ã ‡∞∏‡∞π‡∞æ‡∞Ø‡∞™‡∞°‡∞ó‡∞≤‡∞®‡±Å:",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞®‡∞ø‡∞∞‡±ç‡∞¶‡∞ø‡∞∑‡±ç‡∞ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞à ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞Ö‡∞Ç‡∞∂‡∞æ‡∞≤ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞ó‡∞≤‡∞®‡±Å:",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞¶‡±ä‡∞∞‡∞ï‡∞≤‡±á‡∞¶‡±Å. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞à ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞Ö‡∞Ç‡∞∂‡∞æ‡∞≤ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞ó‡∞≤‡∞®‡±Å:",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞à ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞µ‡∞ø‡∞∑‡∞Ø‡∞æ‡∞≤‡∞§‡±ã ‡∞∏‡∞π‡∞æ‡∞Ø‡∞™‡∞°‡∞ó‡∞≤‡∞®‡±Å:",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞®‡∞ø‡∞∞‡±ç‡∞¶‡∞ø‡∞∑‡±ç‡∞ü ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞ï‡∞æ‡∞®‡±Ä ‡∞à ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞Ö‡∞Ç‡∞∂‡∞æ‡∞≤ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞ó‡∞≤‡∞®‡±Å:"
            ]
        }
        apology = random.choice(apology_responses.get(language, apology_responses['english']))
        suggestions = response.choices[0].message.content.strip()
        
        return f"{apology}\n\n{suggestions}"
    
    except Exception:
        # Fallback to a simple static response if LLM call fails
        no_knowledge_responses = {
            'english': [
                "Sorry, I couldn't find the information you're looking for. Please try asking about a different topic from the Bhagavad Gita.",
                "I apologize, but I don't have that information in my knowledge base. Please try asking about another topic from the Gita.",
                "Sorry, I couldn't find that specific answer. Please try asking about a different aspect of the Bhagavad Gita.",
                "I don't have that information available. Please try asking about another topic from the Bhagavad Gita.",
                "Sorry, I couldn't find that in my knowledge base. Please try asking about a different topic from the Gita.",
                "I apologize, but I don't have that specific information. Please try asking about another aspect of the Bhagavad Gita.",
                "Sorry, I couldn't find that answer. Please try asking about a different topic from the Bhagavad Gita.",
                "I don't have that information. Please try asking about another topic from the Gita."
            ],
            'telugu': [
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞µ‡±Ü‡∞§‡±Å‡∞ï‡±Å‡∞§‡±Å‡∞®‡±ç‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞®‡∞æ‡∞ï‡±Å ‡∞¶‡±ä‡∞∞‡∞ï‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞™‡±ç‡∞∞‡∞Ø‡∞§‡±ç‡∞®‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø.",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø.",
                "‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø.",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø.",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø.",
                "‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø.",
                "‡∞®‡∞æ‡∞ï‡±Å ‡∞Ü ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞≤‡±á‡∞¶‡±Å. ‡∞¶‡∞Ø‡∞ö‡±á‡∞∏‡∞ø ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞≤‡±ã‡∞®‡∞ø ‡∞µ‡±á‡∞∞‡±á ‡∞Ö‡∞Ç‡∞∂‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Ö‡∞°‡∞ó‡∞Ç‡∞°‡∞ø."
            ]
        }
        return random.choice(no_knowledge_responses.get(language, no_knowledge_responses['english']))

def get_or_create_session_id(session_id: Optional[str]) -> str:
    """Get existing session_id or create a new one"""
    if not session_id:
        session_id = str(uuid.uuid4())
    
    if session_id not in conversation_history:
        conversation_history[session_id] = []
    
    return session_id

def add_to_conversation_history(session_id: str, user_query: str, bot_response: str, language: str):
    """Add conversation turn to history"""
    conversation_history[session_id].append({
        "timestamp": datetime.now().isoformat(),
        "user_query": user_query,
        "bot_response": bot_response,
        "language": language
    })
    
    # Keep only last 10 conversation turns to avoid memory issues
    if len(conversation_history[session_id]) > 10:
        conversation_history[session_id] = conversation_history[session_id][-10:]

def get_conversation_context(session_id: str, language: str) -> str:
    """Get recent conversation context for the LLM"""
    if session_id not in conversation_history or not conversation_history[session_id]:
        return ""
    
    recent_turns = conversation_history[session_id][-7:]
    
    context_parts = []
    for turn in recent_turns:
        context_parts.append(f"Previous Q: {turn['user_query']}")
        context_parts.append(f"Previous A: {turn['bot_response']}")
    
    context = "\n".join(context_parts)
    
    language_instructions = {
        'english': "Consider the previous conversation context when answering. Reference previous questions and answers when relevant.",
        'telugu': "‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø ‡∞∏‡∞Ç‡∞≠‡∞æ‡∞∑‡∞£ ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞™‡∞∞‡∞ø‡∞ó‡∞£‡∞®‡∞≤‡±ã‡∞ï‡∞ø ‡∞§‡±Ä‡∞∏‡±Å‡∞ï‡±ä‡∞®‡∞ø ‡∞ú‡∞µ‡∞æ‡∞¨‡±Å ‡∞á‡∞µ‡±ç‡∞µ‡∞Ç‡∞°‡∞ø. ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§‡∞Æ‡±à‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞ú‡∞µ‡∞æ‡∞¨‡±Å‡∞≤‡∞®‡±Å ‡∞∏‡±Ç‡∞ö‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø."
    }
    
    instruction = language_instructions.get(language, language_instructions['english'])
    return f"{instruction}\n\nPrevious conversation:\n{context}\n\n"

def get_structured_conversation_messages(session_id: str, language: str) -> list[Dict[str, str]]:
    """Return structured chat messages from recent history for LLM consumption"""
    if session_id not in conversation_history or not conversation_history[session_id]:
        return []

    # Create comprehensive system prompt that includes conversation context instructions
    language_instructions = {
        'english': "You are a caring and empathetic Bhagavad Gita teacher chatbot. You MUST answer questions ONLY based on the provided document context from Chapter 1-3. You MUST NOT use any external knowledge. If the answer is not in the document context, you MUST respond by saying \"Sorry, the information to answer your question is not in my current knowledge base.\" IMPORTANT: You must understand when the user is asking a follow-up question, even if it's vague like 'are you sure?', 'what do you mean?', or 'tell me more'. Use the previous 7 turns of conversation history to infer context and respond empathetically. When asked 'are you sure?', respond with playful but respectful confirmation like 'Yes, absolutely! üòä Based on the Gita's verses, here's why...' Respond like a caring teacher who understands the user's feelings and intent. Keep answers natural and human, sometimes using 2-3 sentences for clarity. Avoid robotic tone. Be warm, conversational, and empathetic. Use the context to answer the question as accurately as possible. If the context contains relevant information, provide a helpful answer. Only say 'Sorry, this is not in the transcripts' if the context truly doesn't contain any relevant information to answer the question. **Think step-by-step before answering.** Detail your thought process, including how you analyzed the user's query and how you used the context to formulate the answer. CRITICAL: You MUST respond ONLY in English. Do not use any other language.",
        'telugu': "‡∞Æ‡±Ä‡∞∞‡±Å ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞∏‡∞π‡∞æ‡∞Ø‡∞™‡∞°‡±á ‡∞∏‡∞Ç‡∞§‡±ã‡∞∑‡∞ï‡∞∞‡∞Æ‡±à‡∞® ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡∞æ‡∞®‡±Å‡∞≠‡±Ç‡∞§‡∞ø‡∞™‡∞∞‡±Å‡∞°‡±à‡∞® ‡∞ö‡∞æ‡∞ü‡±ç‚Äå‡∞¨‡∞æ‡∞ü‡±ç. ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞ø‡∞® ‡∞™‡∞§‡±ç‡∞∞ ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞≤‡±ã ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡∞ï‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞á‡∞µ‡±ç‡∞µ‡∞æ‡∞≤‡∞ø. ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞¨‡∞æ‡∞π‡±ç‡∞Ø ‡∞ú‡±ç‡∞û‡∞æ‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ï‡±Ç‡∞°‡∞¶‡±Å. ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞™‡∞§‡±ç‡∞∞ ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç‡∞≤‡±ã ‡∞≤‡±á‡∞ï‡∞™‡±ã‡∞§‡±á, \"‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞Æ‡±Ä ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞ï‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞á‡∞µ‡±ç‡∞µ‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞Ç ‡∞®‡∞æ ‡∞™‡±ç‡∞∞‡∞∏‡±ç‡∞§‡±Å‡∞§ ‡∞ú‡±ç‡∞û‡∞æ‡∞® ‡∞Ü‡∞ß‡∞æ‡∞∞‡∞Ç‡∞≤‡±ã ‡∞≤‡±á‡∞¶‡±Å\" ‡∞Ö‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞æ‡∞≤‡∞ø. ‡∞Æ‡±Å‡∞ñ‡±ç‡∞Ø‡∞Æ‡±à‡∞®‡∞¶‡∞ø: ‡∞µ‡∞ø‡∞®‡∞ø‡∞Ø‡±ã‡∞ó‡∞¶‡∞æ‡∞∞‡±Å ‡∞´‡∞æ‡∞≤‡±ã-‡∞Ö‡∞™‡±ç ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞® ‡∞Ö‡∞°‡±Å‡∞ó‡±Å‡∞§‡±Å‡∞®‡±ç‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞æ‡∞≤‡∞ø, ‡∞Ö‡∞¶‡∞ø '‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞Ç‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ?', '‡∞Æ‡±Ä‡∞∞‡±Å ‡∞è‡∞Æ‡∞ø ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?', ‡∞≤‡±á‡∞¶‡∞æ '‡∞Æ‡∞∞‡∞ø‡∞Ç‡∞§ ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞Ç‡∞°‡∞ø' ‡∞µ‡∞Ç‡∞ü‡∞ø‡∞¶‡∞ø ‡∞Ö‡∞Ø‡∞ø‡∞®‡∞æ. ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞Æ‡±Å‡∞®‡±Å‡∞™‡∞ü‡∞ø 7 ‡∞∏‡∞Ç‡∞≠‡∞æ‡∞∑‡∞£‡∞≤‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡∞æ‡∞®‡±Å‡∞≠‡±Ç‡∞§‡∞ø‡∞§‡±ã ‡∞™‡±ç‡∞∞‡∞§‡∞ø‡∞∏‡±ç‡∞™‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø. '‡∞Æ‡±Ä‡∞∞‡±Å ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞Ç‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ?' ‡∞Ö‡∞®‡∞ø ‡∞Ö‡∞°‡∞ø‡∞ó‡∞ø‡∞®‡∞™‡±ç‡∞™‡±Å‡∞°‡±Å, '‡∞Ö‡∞µ‡±Å‡∞®‡±Å, ‡∞ñ‡∞ö‡±ç‡∞ö‡∞ø‡∞§‡∞Ç‡∞ó‡∞æ! üòä ‡∞ó‡±Ä‡∞§ ‡∞∂‡±ç‡∞≤‡±ã‡∞ï‡∞æ‡∞≤ ‡∞Ü‡∞ß‡∞æ‡∞∞‡∞Ç‡∞ó‡∞æ, ‡∞á‡∞¶‡∞ø ‡∞é‡∞Ç‡∞¶‡±Å‡∞ï‡±Å...' ‡∞µ‡∞Ç‡∞ü‡∞ø ‡∞Ü‡∞°‡±Å‡∞§‡±Ç ‡∞ï‡∞æ‡∞®‡±Ä ‡∞ó‡±å‡∞∞‡∞µ‡∞™‡±ç‡∞∞‡∞¶‡∞Æ‡±à‡∞® ‡∞®‡∞ø‡∞∞‡±ç‡∞ß‡∞æ‡∞∞‡∞£‡∞§‡±ã ‡∞™‡±ç‡∞∞‡∞§‡∞ø‡∞∏‡±ç‡∞™‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø. ‡∞µ‡∞ø‡∞®‡∞ø‡∞Ø‡±ã‡∞ó‡∞¶‡∞æ‡∞∞‡±Å ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞≠‡∞æ‡∞µ‡∞®‡∞≤‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞â‡∞¶‡±ç‡∞¶‡±á‡∞∂‡±ç‡∞Ø‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞Ö‡∞∞‡±ç‡∞•‡∞Ç ‡∞ö‡±á‡∞∏‡±Å‡∞ï‡±Å‡∞®‡±á ‡∞∏‡∞Ç‡∞§‡±ã‡∞∑‡∞ï‡∞∞‡∞Æ‡±à‡∞® ‡∞ó‡±Å‡∞∞‡±Å‡∞µ‡±Å‡∞≤‡∞æ ‡∞™‡±ç‡∞∞‡∞§‡∞ø‡∞∏‡±ç‡∞™‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø. ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞æ‡∞≤‡∞®‡±Å ‡∞∏‡∞π‡∞ú‡∞Æ‡±à‡∞®‡∞µ‡∞ø‡∞ó‡∞æ ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞Æ‡∞æ‡∞®‡∞µ‡±Ä‡∞Ø‡∞Ç‡∞ó‡∞æ ‡∞â‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞ï‡±ä‡∞®‡±ç‡∞®‡∞ø‡∞∏‡∞æ‡∞∞‡±ç‡∞≤‡±Å ‡∞∏‡±ç‡∞™‡∞∑‡±ç‡∞ü‡∞§ ‡∞ï‡±ã‡∞∏‡∞Ç 2-3 ‡∞µ‡∞æ‡∞ï‡±ç‡∞Ø‡∞æ‡∞≤‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø. ‡∞Ø‡∞æ‡∞Ç‡∞§‡±ç‡∞∞‡∞ø‡∞ï ‡∞∏‡±ç‡∞µ‡∞∞‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞®‡∞ø‡∞µ‡∞æ‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø. ‡∞µ‡±Ü‡∞ö‡±ç‡∞ö‡∞¶‡∞®‡∞Ç, ‡∞∏‡∞Ç‡∞≠‡∞æ‡∞∑‡∞£ ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡∞æ‡∞®‡±Å‡∞≠‡±Ç‡∞§‡∞ø‡∞§‡±ã ‡∞â‡∞Ç‡∞°‡∞Ç‡∞°‡∞ø. ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞ï‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞á‡∞µ‡±ç‡∞µ‡∞Ç‡∞°‡∞ø. ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞ü‡±á, ‡∞∏‡∞π‡∞æ‡∞Ø‡∞ï‡∞∞‡∞Æ‡±à‡∞® ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞Ö‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø. ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞Ç ‡∞®‡∞ø‡∞ú‡∞Ç‡∞ó‡∞æ ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞ï‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞á‡∞µ‡±ç‡∞µ‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞Ç‡∞¨‡∞Ç‡∞ß‡∞ø‡∞§ ‡∞∏‡∞Æ‡∞æ‡∞ö‡∞æ‡∞∞‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞ï‡∞≤‡∞ø‡∞ó‡∞ø ‡∞â‡∞Ç‡∞°‡∞ï‡∞™‡±ã‡∞§‡±á ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á '‡∞ï‡±ç‡∞∑‡∞Æ‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø, ‡∞á‡∞¶‡∞ø ‡∞ü‡±ç‡∞∞‡∞æ‡∞®‡±ç‚Äå‡∞∏‡±ç‡∞ï‡±ç‡∞∞‡∞ø‡∞™‡±ç‡∞ü‡±ç‚Äå‡∞≤‡∞≤‡±ã ‡∞≤‡±á‡∞¶‡±Å' ‡∞Ö‡∞®‡∞ø ‡∞ö‡±Ü‡∞™‡±ç‡∞™‡∞Ç‡∞°‡∞ø. **‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞Ç ‡∞á‡∞µ‡±ç‡∞µ‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞Æ‡±Å‡∞Ç‡∞¶‡±Å ‡∞¶‡∞∂‡∞≤‡∞µ‡∞æ‡∞∞‡±Ä‡∞ó‡∞æ ‡∞Ü‡∞≤‡±ã‡∞ö‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø.** ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞µ‡∞ø‡∞®‡∞ø‡∞Ø‡±ã‡∞ó‡∞¶‡∞æ‡∞∞‡±Å ‡∞Ø‡±ä‡∞ï‡±ç‡∞ï ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞µ‡∞ø‡∞∂‡±ç‡∞≤‡±á‡∞∑‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å ‡∞Æ‡∞∞‡∞ø‡∞Ø‡±Å ‡∞∏‡∞Æ‡∞æ‡∞ß‡∞æ‡∞®‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞∞‡±Ç‡∞™‡±ä‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞°‡∞æ‡∞®‡∞ø‡∞ï‡∞ø ‡∞∏‡∞Ç‡∞¶‡∞∞‡±ç‡∞≠‡∞æ‡∞®‡±ç‡∞®‡∞ø ‡∞é‡∞≤‡∞æ ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞æ‡∞∞‡±Å ‡∞µ‡∞Ç‡∞ü‡∞ø ‡∞Æ‡±Ä ‡∞Ü‡∞≤‡±ã‡∞ö‡∞® ‡∞™‡±ç‡∞∞‡∞ï‡±ç‡∞∞‡∞ø‡∞Ø‡∞®‡±Å ‡∞µ‡∞ø‡∞µ‡∞∞‡∞ø‡∞Ç‡∞ö‡∞Ç‡∞°‡∞ø. ‡∞ï‡±ç‡∞≤‡∞ø‡∞∑‡±ç‡∞ü‡∞Æ‡±à‡∞®‡∞¶‡∞ø: ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å‡∞≤‡±ã ‡∞Æ‡∞æ‡∞§‡±ç‡∞∞‡∞Æ‡±á ‡∞™‡±ç‡∞∞‡∞§‡∞ø‡∞∏‡±ç‡∞™‡∞Ç‡∞¶‡∞ø‡∞Ç‡∞ö‡∞æ‡∞≤‡∞ø. ‡∞á‡∞Ç‡∞ó‡±ç‡∞≤‡±Ä‡∞∑‡±Å ‡∞≤‡±á‡∞¶‡∞æ ‡∞á‡∞§‡∞∞ ‡∞≠‡∞æ‡∞∑‡∞≤‡∞®‡±Å ‡∞â‡∞™‡∞Ø‡±ã‡∞ó‡∞ø‡∞Ç‡∞ö‡∞ï‡∞Ç‡∞°‡∞ø."
    }
    
    system_prompt = language_instructions.get(language, language_instructions['english'])
    recent_turns = conversation_history[session_id][-7:]
    messages: list[Dict[str, str]] = []
    messages.append({"role": "system", "content": system_prompt})
    for turn in recent_turns:
        messages.append({"role": "user", "content": turn["user_query"]})
        messages.append({"role": "assistant", "content": turn["bot_response"]})
    return messages

def detect_language_and_greeting(text: str) -> tuple[bool, str]:
    """Detect if text is a greeting and return the detected language (English/Telugu only)"""
    text_lower = text.lower().strip()
    try:
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": "You are a language detection expert. Analyze the given text and determine: 1) Is this a greeting or casual conversation starter? 2) What language is it in? Respond with only: GREETING:true/false,LANGUAGE:english/telugu. If the text is in Hindi, Kannada, Marathi, or any other language, respond with LANGUAGE:english."
                },
                {"role": "user", "content": text}
            ],
            max_tokens=30,
            temperature=0.1
        )
        result = response.choices[0].message.content.lower()
        is_greeting = "greeting:true" in result
        language = "english"
        if "language:telugu" in result:
            language = "telugu"
        return is_greeting, language
    except Exception:
        return detect_greeting_fallback(text)

def detect_greeting_fallback(text: str) -> tuple[bool, str]:
    """Fallback greeting detection using simple patterns (English/Telugu only)"""
    text_lower = text.lower().strip()
    greeting_patterns = [
        (r'\b(hi|hello|hey|good morning|good afternoon|good evening|how are you|how do you do|jai srimannarayana)\b', 'english'),
        (r'\b(namaskaram|namaskaramu|bagunnara|ela unnaru|ela unnavu|jai srimannarayana)\b', 'telugu')
    ]
    for pattern, language in greeting_patterns:
        if re.search(pattern, text_lower):
            return True, language
    detected_language = detect_language_from_content(text)
    return False, detected_language

def detect_language_from_content(text: str) -> str:
    """Detect language from text content using Unicode ranges and common words (English/Telugu only)"""
    text_lower = text.lower()
    if any('\u0c00' <= char <= '\u0c7f' for char in text):
        return 'telugu'
    telugu_words = ['telugu', 'andhra', 'telangana', 'garu', 'unnaru', 'unnavu', 'ela', 'baga', 'bagunnara']
    if any(word in text_lower for word in telugu_words):
        return 'telugu'
    # For any other language (Hindi, Kannada, Marathi, etc.), default to English
    return 'english'

def get_language_restriction_response() -> str:
    """Get a polite response when user queries in unsupported language"""
    apology_variations = [
        "Sorry, I can only reply in English or Telugu right now. Please try asking your question in one of these languages! üòä",
        "I apologize, but I'm currently limited to English and Telugu responses. Could you please rephrase your question in one of these languages? üôè",
        "Sorry, I can only communicate in English or Telugu at the moment. Feel free to ask your Bhagavad Gita question in either language! üòä",
        "I'd love to help, but I can only respond in English or Telugu right now. Please try your question in one of these languages! üôè",
        "Sorry, I can only reply in English or Telugu currently. Please ask your question in one of these languages and I'll be happy to help! üòä",
        "I apologize, but I'm only able to respond in English or Telugu. Could you please ask your question in one of these languages? üôè",
        "Sorry, I can only help in English or Telugu right now. Please rephrase your question in one of these languages! üòä",
        "I'd be happy to assist, but I can only reply in English or Telugu. Please try asking your question in one of these languages! üôè"
    ]
    return random.choice(apology_variations)

def get_greeting_response(language: str, user_query: str = "") -> str:
    """Get natural greeting response in the detected language (English/Telugu only)"""
    try:
        openai_client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {
                    "role": "system",
                    "content": f"You are a friendly Bhagavad Gita chatbot. The user greeted you in {language}. Respond naturally with 'Jai Srimannarayana' and acknowledge their greeting, then ask how you can help them learn about the Bhagavad Gita. Be conversational and warm. Respond in {language}."
                },
                {"role": "user", "content": user_query}
            ],
            max_tokens=80,
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception:
        responses = {
            'english': [
                "Jai Srimannarayana! üôè I'm doing well, thank you for asking! How can I help you learn about the Bhagavad Gita today?",
                "Jai Srimannarayana! üôè I'm fine, thanks for your concern! What would you like to know about the Bhagavad Gita?",
                "Jai Srimannarayana! üôè All is well, thank you! How may I assist you with the teachings of the Bhagavad Gita?",
                "Jai Srimannarayana! üôè I'm doing great! What questions do you have about Krishna's wisdom in the Gita?",
                "Jai Srimannarayana! üôè Everything is wonderful, thank you! How can I help you explore the Bhagavad Gita today?"
            ],
            'telugu': [
                "‡∞ú‡±à ‡∞∂‡±ç‡∞∞‡±Ä‡∞Æ‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ‡∞Ø‡∞£! üôè ‡∞®‡±á‡∞®‡±Å ‡∞¨‡∞æ‡∞ó‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å, ‡∞Ö‡∞°‡∞ø‡∞ó‡∞ø‡∞®‡∞Ç‡∞¶‡±Å‡∞ï‡±Å ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å! ‡∞®‡±á‡∞®‡±Å ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞®‡±á‡∞∞‡±ç‡∞ö‡±Å‡∞ï‡±ã‡∞µ‡∞°‡∞Ç‡∞≤‡±ã ‡∞é‡∞≤‡∞æ ‡∞∏‡∞π‡∞æ‡∞Ø‡∞™‡∞°‡∞ó‡∞≤‡∞®‡±Å?",
                "‡∞ú‡±à ‡∞∂‡±ç‡∞∞‡±Ä‡∞Æ‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ‡∞Ø‡∞£! üôè ‡∞®‡±á‡∞®‡±Å ‡∞ö‡∞ï‡±ç‡∞ï‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å, ‡∞Æ‡±Ä ‡∞Ü‡∞Ç‡∞¶‡±ã‡∞≥‡∞®‡∞ï‡±Å ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å! ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§ ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡±Ä‡∞∞‡±Å ‡∞è‡∞Æ‡∞ø ‡∞§‡±Ü‡∞≤‡±Å‡∞∏‡±Å‡∞ï‡±ã‡∞µ‡∞æ‡∞≤‡∞®‡±Å‡∞ï‡±Å‡∞Ç‡∞ü‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞∞‡±Å?",
                "‡∞ú‡±à ‡∞∂‡±ç‡∞∞‡±Ä‡∞Æ‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ‡∞Ø‡∞£! üôè ‡∞Ö‡∞®‡±ç‡∞®‡±Ä ‡∞¨‡∞æ‡∞ó‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø, ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å! ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§ ‡∞¨‡±ã‡∞ß‡∞®‡∞≤‡∞≤‡±ã ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞®‡±á‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞∏‡∞π‡∞æ‡∞Ø‡∞™‡∞°‡∞ó‡∞≤‡∞®‡±Å?",
                "‡∞ú‡±à ‡∞∂‡±ç‡∞∞‡±Ä‡∞Æ‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ‡∞Ø‡∞£! üôè ‡∞®‡±á‡∞®‡±Å ‡∞ö‡∞æ‡∞≤‡∞æ ‡∞¨‡∞æ‡∞ó‡±Å‡∞®‡±ç‡∞®‡∞æ‡∞®‡±Å! ‡∞ó‡±Ä‡∞§‡∞≤‡±ã ‡∞ï‡±É‡∞∑‡±ç‡∞£‡±Å‡∞°‡∞ø ‡∞ú‡±ç‡∞û‡∞æ‡∞®‡∞Ç ‡∞ó‡±Å‡∞∞‡∞ø‡∞Ç‡∞ö‡∞ø ‡∞Æ‡±Ä ‡∞™‡±ç‡∞∞‡∞∂‡±ç‡∞®‡∞≤‡±Å ‡∞è‡∞Æ‡∞ø‡∞ü‡∞ø?",
                "‡∞ú‡±à ‡∞∂‡±ç‡∞∞‡±Ä‡∞Æ‡∞®‡±ç‡∞®‡∞æ‡∞∞‡∞æ‡∞Ø‡∞£! üôè ‡∞Ö‡∞®‡±ç‡∞®‡±Ä ‡∞Ö‡∞¶‡±ç‡∞≠‡±Å‡∞§‡∞Ç‡∞ó‡∞æ ‡∞â‡∞®‡±ç‡∞®‡∞æ‡∞Ø‡∞ø, ‡∞ß‡∞®‡±ç‡∞Ø‡∞µ‡∞æ‡∞¶‡∞æ‡∞≤‡±Å! ‡∞à‡∞∞‡±ã‡∞ú‡±Å ‡∞≠‡∞ó‡∞µ‡∞¶‡±ç‡∞ó‡±Ä‡∞§‡∞®‡±Å ‡∞Ö‡∞®‡±ç‡∞µ‡±á‡∞∑‡∞ø‡∞Ç‡∞ö‡∞°‡∞Ç‡∞≤‡±ã ‡∞Æ‡±Ä‡∞ï‡±Å ‡∞®‡±á‡∞®‡±Å ‡∞é‡∞≤‡∞æ ‡∞∏‡∞π‡∞æ‡∞Ø‡∞™‡∞°‡∞ó‡∞≤‡∞®‡±Å?"
            ]
        }
        language_responses = responses.get(language, responses['english'])
        return random.choice(language_responses)

@router.post("/")
def chat_post(request: ChatRequest, fastapi_request: Request, fastapi_response: Response):
    """Chat endpoint that accepts POST requests with JSON body"""
    incoming_session_id = request.session_id or fastapi_request.headers.get("X-Session-Id") or fastapi_request.cookies.get("session_id")
    result = process_chat_query(request.query, incoming_session_id)
    if result and isinstance(result, dict) and result.get("session_id"):
        fastapi_response.set_cookie(
            key="session_id",
            value=result["session_id"],
            httponly=False,
            samesite="lax",
            max_age=60 * 60 * 24 * 7
        )
    return result

@router.get("/")
def chat_get(
    fastapi_request: Request,
    fastapi_response: Response,
    query: str = Query(..., description="Your question about the Bhagavad Gita"),
    session_id: Optional[str] = Query(None, description="Session ID for conversation history")
):
    """Chat endpoint that accepts GET requests with query parameter"""
    incoming_session_id = session_id or fastapi_request.headers.get("X-Session-Id") or fastapi_request.cookies.get("session_id")
    result = process_chat_query(query, incoming_session_id)
    if result and isinstance(result, dict) and result.get("session_id"):
        fastapi_response.set_cookie(
            key="session_id",
            value=result["session_id"],
            httponly=False,
            samesite="lax",
            max_age=60 * 60 * 24 * 7
        )
    return result

def process_chat_query(query: str, session_id: Optional[str] = None):
    """Process the chat query and return response with conversation history"""
    try:
        if not query.strip():
            raise HTTPException(status_code=400, detail="Query cannot be empty")
        
        session_id = get_or_create_session_id(session_id)
        
        # The topics_keywords and corresponding if block have been removed
        
        is_greeting_detected, detected_language = detect_language_and_greeting(query)
        
        # Check if the detected language is supported (English/Telugu only)
        if detected_language not in ['english', 'telugu']:
            answer = get_language_restriction_response()
            add_to_conversation_history(session_id, query, answer, 'english')
            return {
                "answer": answer,
                "confidence": 2.0,
                "sources": [],
                "session_id": session_id
            }
        
        conversation_context = get_conversation_context(session_id, detected_language)
        
        if is_greeting_detected:
            answer = get_greeting_response(detected_language, query)
            add_to_conversation_history(session_id, query, answer, detected_language)
            return {
                "answer": answer,
                "confidence": 2.0,
                "sources": [],
                "session_id": session_id
            }
        
        augmented_query = f"{conversation_context}\nCurrent Question: {query}" if conversation_context else query
        query_vector = embeddings.embed_text(augmented_query)
        
        results = pinecone_client.index.query(
            vector=query_vector,
            top_k=3,
            include_metadata=True
        )

        if not results.matches or results.matches[0].score < 0.5:
            answer = get_no_knowledge_response(detected_language, query)
            add_to_conversation_history(session_id, query, answer, detected_language)
            return {
                "answer": answer,
                "confidence": results.matches[0].score if results.matches else 0,
                "sources": [],
                "session_id": session_id
            }
        
        context_chunks = []
        for match in results.matches:
            if match.score > 0.4:
                context_chunks.append({
                    "text": match.metadata["text"],
                    "chunk_id": match.metadata.get("chunk_id", "unknown"),
                    "score": match.score
                })
        
        context = " ".join([chunk["text"] for chunk in context_chunks])
        
        structured_messages = get_structured_conversation_messages(session_id, detected_language)
        answer = llm.generate_answer_with_language_structured(
            query=query,
            context=context,
            language=detected_language,
            history_messages=structured_messages
        )
        
        # Add to conversation history
        add_to_conversation_history(session_id, query, answer, detected_language)
        
        return {
            "answer": answer,
            "confidence": results.matches[0].score,
            "sources": context_chunks[:3],
            "session_id": session_id
        }
    
    except Exception as e:
        # Log the error for debugging
        print(f"Error in process_chat_query: {str(e)}")
        print(f"Error type: {type(e).__name__}")
        import traceback
        print(f"Traceback: {traceback.format_exc()}")
        
        # Return a more informative error response
        return {
            "answer": f"Sorry, an error occurred while processing your query: {str(e)}",
            "confidence": 0.0,
            "sources": [],
            "session_id": session_id if 'session_id' in locals() else "error"
        }

@router.get("/history")
def get_chat_history(session_id: str = Query(..., description="Session ID to retrieve history for")):
    """Get conversation history for a session"""
    if session_id not in conversation_history:
        return {"history": []}
    
    history = conversation_history[session_id][-10:]
    return {"history": history}

@router.get("/health")
def health_check():
    """Health check endpoint"""
    return {"status": "healthy", "message": "Chat service is running"}

@router.get("/debug")
def debug_endpoint():
    """Debug endpoint to check service status and identify issues"""
    debug_info = {
        "status": "debug",
        "environment_check": {},
        "service_status": {},
        "error_details": []
    }
    
    try:
        # Check environment variables
        debug_info["environment_check"] = {
            "openai_key_set": bool(os.getenv("OPENAI_API_KEY")),
            "pinecone_key_set": bool(os.getenv("PINECONE_API_KEY")),
            "pinecone_index": os.getenv("PINECONE_INDEX", "not_set"),
            "supabase_url_set": bool(os.getenv("SUPABASE_URL")),
            "supabase_key_set": bool(os.getenv("SUPABASE_ANON_KEY"))
        }
        
        # Test Pinecone connection
        try:
            pinecone_status = pinecone_client.index.describe_index_stats()
            debug_info["service_status"]["pinecone"] = "connected"
            debug_info["service_status"]["pinecone_vector_count"] = pinecone_status.total_vector_count
        except Exception as e:
            debug_info["service_status"]["pinecone"] = f"error: {str(e)}"
            debug_info["error_details"].append(f"Pinecone error: {str(e)}")
        
        # Test OpenAI connection
        try:
            test_embedding = embeddings.embed_text("test")
            debug_info["service_status"]["openai_embeddings"] = "working"
        except Exception as e:
            debug_info["service_status"]["openai_embeddings"] = f"error: {str(e)}"
            debug_info["error_details"].append(f"OpenAI embeddings error: {str(e)}")
        
        # Test LLM
        try:
            test_response = llm.generate_answer("test", "test context")
            debug_info["service_status"]["openai_llm"] = "working"
        except Exception as e:
            debug_info["service_status"]["openai_llm"] = f"error: {str(e)}"
            debug_info["error_details"].append(f"OpenAI LLM error: {str(e)}")
            
    except Exception as e:
        debug_info["error_details"].append(f"General error: {str(e)}")
    
    return debug_info